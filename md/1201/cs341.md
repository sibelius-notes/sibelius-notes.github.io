---
title: CS 341 - Algorithms (post mid)
layout: mdtoc
---
post mid material: starting from graph algorithms. Notes are a mix of different offerings...

[pseudocode.js](https://github.com/SaswatPadhi/pseudocode.js/tree/master)

# Graph Algorithms
## Basic
directed, adjacent, incident, indegree, outdegree, path, cycle, tree, connected, connected component.

A **simple** path does not repeat vertices. A **simple** cycle does not repeat vertices.


Our algorithms will be for abstract graphs. Two ways to represent:
- adjacency matrix: space &#92;(O(n^2) &#92;)
- adjacency lists: space &#92;(O(n+m) &#92;)

|basic operations  | adj matrix| adj lists|
| :------------- | :------------- | :-- |
| list &#92;(v &#92;)'s neighbours| &#92;(\Theta(n) &#92;) | &#92;(\Theta(1+\deg(v)) &#92;)|
| list all edges | &#92;(\Theta(n^2) &#92;) | &#92;(\Theta(n+m) &#92;) |
| is &#92;((u,v)\in E &#92;) | &#92;(\Theta(1) &#92;) | &#92;(\Theta(1+\deg(u)) &#92;) |

where the last complexity &#92;(\Theta(1+\deg(u)) &#92;)  can be reduced to &#92;(O(1) &#92;)
with (sophisiticated) hashing.

For algorithm in this course, we will use adjacency lists.

## Exploring Graphs

### BFS
Cautious search: check everything one edge away, then two...

It takes &#92;(O(n+m) &#92;) time: we explore each vertex once and check all incident edges: &#92;(O(n+\sum_{v\in V} \deg(v))=O(n+m) &#92;). Recall &#92;(\sum_{v\in V} \deg(v) = 2m &#92;).

**Properties**
- The parent pointers create a directed tree.
- &#92;(u &#92;) is connected to &#92;(v_0 &#92;) iff BFS from &#92;(v_0 &#92;) reaches &#92;(u &#92;).
- The level of a vertex &#92;(v &#92;) = length of shortest path from &#92;(v_0 &#92;) to &#92;(v &#92;).

**Consequences**
1. BFS from &#92;(v_0 &#92;) finds the connected component of &#92;(v_0 &#92;)
2. BFS finds shortest paths (# edges) from &#92;(v_0 &#92;)

**BFS to test bipartiteness**
definition omitted... it cannot have an odd cycle.

### DFS
[Extra notes from CMU](https://www.cs.cmu.edu/~15451-f17/lectures/lec11-DFS-strong-components.pdf)

Bold search: go as far as you can; when there's nothing new to discover, retrace your steps to find sth new.

Note: orders depend on order in adjacency list. Use a stack to store vertices that have been discovered but must still be explored. So we use a recursive algorithm where stack is implicit.

Runtime: &#92;(O(n+m) &#92;)

All non-tree edges join ancestor and descendant.

Here we abbr: &#92;(d(v)=discovers(v), f(v)=finish(v) &#92;). If &#92;(d(v)<d(u) &#92;) then we have only two possiblities: either nested or disjoint
```
[       ]       [       ]           OR      [        [       ]       ]
d(v)    f(v)    d(u)    f(u)                d(v)     d(u)    f(u)    f(v)
```
because interval &#92;(d(v),f(v) &#92;) is time on stack.

**DFS to find 2-connected components**. Involve some defns of cut-vertex. Not covered in D.R. Stinson's slide.
- **Claim**: The root is a cut vertex iff it has &#92;(> 1&#92;) child.
- **Lemma**: non-root &#92;(v &#92;) is a cut vertex iff &#92;(v &#92;) has a subtree &#92;(T &#92;) with non-tree edge going to an ancestor of &#92;(v &#92;).

Now let's DFS on **Directed Graphs**.
- **tree edges** form a forest of trees.
- **forward edges** are from a vertex to a descendant in the tree.
- **cross edges** are from node a to node b where the subtrees rooted at a and b are disjoint.
- **back edges** are from a vertex to an ancestor.

Then in DFS, we label the edges:
- if u not finished, then (v, u) is back edge
- else if &#92;(d(u) > d(v)&#92;) then (v, u) forward edge
- else if &#92;(d(u) < d(v) &#92;) then (v, u) cross edge

**Applications of DFS**
1. detecting cycles in directed graphs.<br>
**Lemma**: A directed graph has a directed cycle iff DFS has a back edge.
2. Topological sort of a directed **acyclic** (no directed cycle) graph. Edge &#92;((a,b) &#92;) means &#92;(a &#92;) must come before &#92;(b &#92;). (e.g. job scheduling). Find a linear order of vertices satisfying all edges. (possible if no directed cycle).
<br>**One solution**: find vertex &#92;(v &#92;) with no in-edge. Remove &#92;(v &#92;) and repeat. &#92;(O(n+m) &#92;)
<br>**Solution using DFS**: also &#92;(O(n+m) &#92;) use reverse of finish order.
3. Finding strongly connected components in a directed graph.
<br> Strongly connected = for all vertices &#92;(u,v &#92;) there is a path &#92;(u\to v &#92;) and a path &#92;(v\to u &#92;).
<br> **Claim**  Let &#92;(s &#92;) be a vertex. &#92;(G &#92;) is strongly connected if for all vertices &#92;(v &#92;) there is a path &#92;(s\to v &#92;) and a path &#92;(v\to s &#92;).
<br>To test if there's a path &#92;(s\to v\quad \forall v &#92;), do DFS(s). How can we test it there's a path &#92;(v\to s\quad \forall v &#92;)? Reverse edge directions and do DFS(s). **Neat!**.

More generally, a digraph can be partitioned to strongly connected components.

<img src="/pics/DAG.png">

Picture from [here](https://www.cs.cmu.edu/~15451-f17/lectures/lec11-DFS-strong-components.pdf).

Contracting strongly connected components gives acyclic graph.

*How to find strongly connected components?*

Idea: vertices 1 ... n.
<br>Run DFS (vertex ordering resolves what vertex comes next).
<br> Let finish order be &#92;(f_1 f_2 \ldots f_2 &#92;).
<br>&#92;(G^R=G &#92;) with all edges reversed.
<br>run DFS &#92;(G^R &#92;) with vertex order &#92;(f_n f_{n-1}\ldots f_1 &#92;)

Trees in 2nd DFS are exactly the strongly connected components.

**Sharir's Algorithm to Find the Strongly Connected Components**
1. Perform a depth-first search of G, recording the finishing times *f[v]* for all vertices v.
2. Construct a directed graph H from G by **reversing** the direction of all edges in G.
3. Perform a depth-first search of H, considering the vertices in **decreasing** order of the values *f[v]* computed in step 1.
4. The strongly connected components of G are the trees in the depth-first forest constructed in step 3.

## Minimum Spanning Tree
Given a graph &#92;(G=(V,E) &#92;) with weights &#92;(\omega:E\to \mathbb R^{\ge 0} &#92;) one the edges find a subset of the edges that connected all the vertices and has minimum weight. The edge subset will be a tree, called the **minimum spanning tree**.

Greedy alg will find min spanning trees with different implementation challenges.
- add cheapest edge first, never build a cycle: Kruskal's alg.
- grow connected graph from one vertex: Prim's alg.

### Kruskal's Algorithm
```
Order edges by weight
    w(e_i) <= w(e_{i+1})
T <- âˆ…
for i = 1..m
    if e_i does not make a cycle with T then
        T <- T âˆª {e_i}
endfor
```
 &#92;(O(m\log m) = O(m\log n) &#92;) to sort edges because &#92;(m\le n^2 &#92;). Then we need to  maintain connected components as we add edges. So it is union-find problem.

#### Union-Find Problem
Maintain a collection of disjoint sets.

Operations:
- `Find(x)` - which set contains element x?
- `Union` - unite two sets.

Every tree contains a leader vertex. We construct an auxiliary array &#92;(L &#92;). From &#92;(v &#92;), follow a directed path &#92;(v\to L[v]\to L[L[v]] \to \ldots &#92;) until we reach &#92;(w &#92;) with &#92;(L[w]=w &#92;). Then &#92;(w=find(v) &#92;).

> Or keep array `S[1..n]`, `S[i]` = component  of element i and keep linked list in each set.

Then `find` is &#92;(O(1) &#92;). Union: must joint 2 linked lists &#92;(O(1) &#92;) and rename one of the two sets. So &#92;(O(n) &#92;) in worst case. BUT renaming smaller set does better! If an element's set number changes, then its set (more than) doubles. This happens &#92;(\le  \log n&#92;)  times. Therefore total renaming work is &#92;(O(n\log n) &#92;). Thus total runtime
&#92;[
    \underbrace{O(m\log n)}_ {sort} +
    \underbrace{O(m)}_ {finds} +
    \underbrace{O(n\log n)}_ {unions}
    = O(m\log n)
&#92;]
Assuming &#92;(G &#92;) is connected, so &#92;(m\ge n-1 &#92;).

### Prim's Algorithm
```
initialize C <- {S}, T <- âˆ…
while C != V
    find min weight edge e = (u, v) from u âˆˆ C to v âˆˆ V \ C
    T <- T âˆª {e}
    C <- C âˆª {v}
end
```


Implementation: We use priority queue as data structure (using min-heap, and we know each operation takes &#92;(O(\log k) &#92;) where &#92;(k &#92;) is # of elements), maitain a set of weighted elements.
- Find and delete min weight element
- insert
- delete

Total cost
&#92;[
    O(n\log m + m\log m+ m\log m)  = O(m\log m) = O(m\log n)
&#92;]

In graph theory, a **cut** is a partition of the vertices of a graph into two disjoint subsets. Any cut determines a cut-set, the set of edges that have one endpoint in each subset of the partition. These edges are said to cross the cut. These edges are called **crossing edges**. Let &#92;(A\subseteq E &#92;). A cut &#92;((S,V\setminus S) &#92;) **respects** the set of edges &#92;(A &#92;) provided that no edges in &#92;(A &#92;) is a crossing edge.

**A general greedy algorithm to find an MST**
```
A <- âˆ…
while |A| < n-1
    (S, V\S) <- a cit that respects A
    let e be a minimum crossing edge
    A <- A âˆª {e}
end
```

## Single Source Shortest paths
**Instance**: A directed graph &#92;(G=(V,E) &#92;), a non-negative weight function &#92;(w: E\to \mathbb R^{\ge 0} &#92;), and a source vertex &#92;(u_0\in V &#92;).

**Find**: For every vertex &#92;(v\in V &#92;), a directed path &#92;(P &#92;) from &#92;(u_0 &#92;) to &#92;(v &#92;) such that &#92;(w(P)=\sum\limits_{e\in P} w(e)&#92;) is minimized.

### Dijkstra's Algorithm
Dijkstra's algorithm requires that the graph have no edge weights &#92;(< 0 &#92;); it works for directed or undirected graphs.

General step:
```
Have tree of shortest paths to all vertices in set B; initially B = {s}

Choose edge (x,y), x âˆˆ B, y âˆ‰ B to minimize d(s, x) + w(x, y).
// Here d(s, x) is distance from s to x, which is known.

d(s,y) <- d_min
add (x,y) to tree (Parent(y) <- x)
```

![](/pics/Dijkstra.png)

picture from Prof. Lubiw's F19 CS341 notes. This is greedy in the sense that we always add the vertex with next min distance from s.

Implmentations:
- want to choose edge leaving B to minimize some values
- could make a heap of edges which has size &#92;(O(m) &#92;)
- More effcient: a heap of vertices

Keep "tentative distance" &#92;(d(v) \quad \forall v\not\in B &#92;)

Total time &#92;(O(n\log n + m\log n)=O(m\log n)  &#92;). Here it takes &#92;(n &#92;) to find min, &#92;(m &#92;) to adjust heap. Actually, there is a fancier "Fibonacci heap" that gives &#92;(O(n\log n+m) &#92;).

**Recall**: negative weight cycles are trouble. Note that negative weight edges in an undirected graph are not allowed, as they would give rise to a negative cycle (consisting of two edges) in the associated directed graph.

## Shortest Paths in a DAG
If G is a DAG, we perform a topological ordering of the vertices, the result is &#92;(v_ 1,\ldots, v_ n &#92;). Recall this is done by DFS. If v vomes before s, there is no path s->v. So throw these vertice away.

```
for i = 1 .. n
    for every edge (v_i, v_j)
        if d_i + w(v_i, v_j) < d_j then
            d_j <- d_i + w(v_i, v_j)
    endfor
endfor
```
This is &#92;(O(n+m) &#92;).

### Bellman-Ford
The Bellman-Ford algorithm solves the single source shortest path problem in any directed graph without negative weight cycles.
It is **the** original application of dynamic programming.

**Note**: if there is a negative weight cycle then shortest paths are not well-defined. Go around the cycle move and move to decrease length of path arbitrarily.

**Idea of DP for shortest paths**: If shortest uv path goes through x, then it consists of shortest ux path + shortest xv path (these are subproblems)

Here we want to compute:
&#92;[
    d_i[v] = \text{ length of the shortest path from $s$ to $v$ that uses $\le i$ edges}
&#92;]
base case: &#92;(d_1[v] = &#92;begin{cases}
0 & &#92;text{if } v=s &#92;&#92;
w(s,v) & &#92;text{if } (s,v)\in E  &#92;&#92;
   \infty& &#92;text{otherwise}
&#92;end{cases} &#92;)

and our recurrence:
&#92;[
    d_i(v)= \min &#92;begin{cases}
    d_{i-1}(v) & &#92;text{(use $\le i-1$ edges) }  &#92;&#92;
     \min\limits_{u \in &#92;left&#92;{ u: (u,v)\in E &#92;right&#92;}} (d_{i-1}(u)+w(u,v))  & &#92;text{(use $i$ edges)}
    &#92;end{cases}
&#92;]

![](/pics/bellman1.png)

Picture (as well as some notes below) from Prof. Blais's F19 CS341 notes. This is &#92;(O(n^3) &#92;). The correctness of this algorithm follows directly from the correctness of the Bellman-Ford algorithm. Recall that the Bellman-Ford algorithm is obtained via the dynamic programming technique by considering the shortest paths from &#92;(s &#92;) to &#92;(v &#92;) that go over only &#92;(1,2,\ldots,n-1 &#92;) edges. We saw in the last lecture that we can store these distances in vectors
&#92;(d_1,d_2,\ldots,d_{n-1} &#92;). It turns out that we don't need to store all these distance vectors separately, and we can simplify the Bellman-Ford algorithm to use a single distance vector that
we update as follows.

![](/pics/bellman2.png)

Note the curious fact that &#92;(i &#92;) does not appear inside the loop. So we actually run after some iterations to update some values... Runtime: &#92;(\Theta(nm) &#92;).

However if we use all this algorithm to APSP(All-Pairs Shortest Paths), runtime would be &#92;(O(n^2m) &#92;).

**Conclusion**: To obtain a more ecient algorithm with the dynamic programming
technique, we need tond other subproblems to solve.

## All-Pairs Shortest Paths
In the all-pairs shortest path (APSP)
problem, we are given a weighted (directed or undirected) graph &#92;(G=(V,E) &#92;) and we must
return the length of the shortest path from &#92;(u\to v &#92;) for every &#92;(u,v\in V &#92;).

**A Dynamic Programming Approach**: &#92;(L_m[i,j] &#92;) denotes the minimum-weight &#92;((i,j) &#92;)-path having at most &#92;(m &#92;) edges. For &#92;(m\ge 2 &#92;),
&#92;[
    L_ m[i,j] = \min &#92;left&#92;{ L_ {m-1}[i,k]+ L_ 1[k,j]: 1\le k\le n &#92;right&#92;}
&#92;]
Complexity: &#92;(O(n^4) &#92;).

### Floyd-Warshall
To design the Bellman{Ford algorithm, we noted that the problem of computing the distance
from u to v is much easier if we restrict the set of paths that we consider. This observation
led us to consider only paths that go through a small number of edges. But that's not
the only way to restrict our attention to some of the paths only. In particular, instead of
limiting the number of edges that the paths can use, we can limit the set of intermediate
nodes that those paths can traverse. Let us label the vertices in &#92;(V &#92;) as &#92;(v_1,\ldots,v_n &#92;). Then
for each &#92;(0\le k\le n &#92;), we can define
&#92;[
    dist_ k[u,v] = &#92;begin{aligned}
    \text{minimum weight of any path from $u$ to $v$ that} &#92;&#92;
    \text{only uses &#92;(v_1,\ldots,v_k &#92;) as intermediate nodes.}
    &#92;end{aligned}
&#92;]
Now base case (&#92;(dist_ 0[u,v] &#92;)) is simple: if edge then just the weight, otherwise &#92;(\infty &#92;). Recurrence:
&#92;[
    dist_ k[u,v] = \min &#92;begin{cases}
    dist_ {k-1}[u, v_ k] + dist_ {k-1} [v_ k,v]&#92;&#92;
    dist_ {k-1}[u,v]
    &#92;end{cases}
&#92;]
Combine all these, we get the algorithm with &#92;(\Theta(n^3) &#92;).

![](/pics/floyd.png)

![](/pics/floyd-exercise.png)

> Anna page 129, Eric page 84, Stinson page 261
