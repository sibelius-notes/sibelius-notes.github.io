---
title: Applied Probability
layout: markdown
---

<div class="content">

<h1>Applied Probability</h1>

<div class="md-toc content">
    <p class="md-toc-content">
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lecture-2">Lecture 2</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#introduction">Introduction</a>
    </span>
    <span class="md-toc-item md-toc-h3">
        <a class="md-toc-inner" href="#basic-concepts-of-prob-theory">Basic concepts of prob theory</a>
    </span>
    <span class="md-toc-item md-toc-h4">
        <a class="md-toc-inner" href="#**prob-model**">**Prob model**</a>
    </span>
    <span class="md-toc-item md-toc-h4">
        <a class="md-toc-inner" href="#**indepdence**">**Indepdence**</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lecture-3">Lecture 3</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#conditional-prob">conditional prob</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#bayesian-formula">Bayesian formula</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#random-variables">random variables</a>
    </span>
    <span class="md-toc-item md-toc-h3">
        <a class="md-toc-inner" href="#two-types-of-rvs-in-this-course">two types of rvs in this course</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-4">Lec 4</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#bernulli-trails">Bernulli Trails</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#binomial-rv">Binomial rv</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#geometric-rv">Geometric rv</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-5">Lec 5</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#negtive-binomial">Negtive Binomial</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#possion-rv:-pois-($\lambda$)">Possion rv: pois ($\lambda$)</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#exponential-rv:-$\exp(\lambda)$">Exponential rv: $\exp(\lambda)$</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#expectation-&amp;-variance">Expectation &amp; Variance</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-6">Lec 6</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#indicator-rvs">indicator rvs</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#chapter-2-waiting-time-rcs">chapter 2 waiting time rcs</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-7">Lec 7</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#chapter-3-conditional-expectation">Chapter 3 Conditional expectation</a>
    </span>
    <span class="md-toc-item md-toc-h3">
        <a class="md-toc-inner" href="#3.1-joint-rvs">3.1 joint rvs</a>
    </span>
    <span class="md-toc-item md-toc-h4">
        <a class="md-toc-inner" href="#joint-discrete:-x-&amp;-y">Joint discrete: X &amp; Y</a>
    </span>
    <span class="md-toc-item md-toc-h4">
        <a class="md-toc-inner" href="#joint-continuous">joint continuous</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-8">Lec 8</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#3.2-conditional-distribution-&amp;-conditional-expectation">3.2 Conditional distribution &amp; Conditional expectation</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-9">Lec 9</a>
    </span>
    <span class="md-toc-item md-toc-h3">
        <a class="md-toc-inner" href="#properties-of-conditional-expectation">properties of conditional expectation</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#3.3-calculating-expectation-by-conditioning">3.3 Calculating expectation by conditioning</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-10">Lec 10</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#3.4-computing-prob-by-conditioning">3.4 computing prob by conditioning</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-12">Lec 12</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#3.5-calculating-variance-by-conditioning">3.5 Calculating variance by conditioning</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-13">Lec 13</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#ch-4-probability-generating-function-(pgf)">ch 4 probability generating function (pgf)</a>
    </span>
    <span class="md-toc-item md-toc-h3">
        <a class="md-toc-inner" href="#4.1-generating-function-(gf)">4.1 generating function (gf)</a>
    </span>
    <span class="md-toc-item md-toc-h3">
        <a class="md-toc-inner" href="#power-series">power series</a>
    </span>
    <span class="md-toc-item md-toc-h4">
        <a class="md-toc-inner" href="#commonly-used-power-series">commonly used power series</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-14">Lec 14</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#4.2-probablity-generating-function-(pgf)">4.2 Probablity generating function (pgf)</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-15">Lec 15</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-16">Lec 16</a>
    </span>
    <span class="md-toc-item md-toc-h2">
        <a class="md-toc-inner" href="#4.3-simple-random-walk">4.3 Simple random walk</a>
    </span>
    <span class="md-toc-item md-toc-h1">
        <a class="md-toc-inner" href="#lec-17">Lec 17</a>
    </span>
    </p>
</div><h1 id="lecture-2"><a class="header-link" href="#lecture-2"></a>Lecture 2</h1>
<ul class="list">
<li>Prob model</li>
<li>independence</li>
</ul>
<h2 id="introduction"><a class="header-link" href="#introduction"></a>Introduction</h2>
<h3 id="basic-concepts-of-prob-theory"><a class="header-link" href="#basic-concepts-of-prob-theory"></a>Basic concepts of prob theory</h3>
<h4 id="**prob-model**"><a class="header-link" href="#**prob-model**"></a><strong>Prob model</strong></h4>
<p>it has three components: sample space, events, prob function</p>
<ul class="list">
<li><strong>Sample space</strong>: all possible outcomes of a random experiment</li>
</ul>
<p>Eg: toss a die once. sample space = {1,2,3,4,5,6}</p>
<p><strong>Events</strong>: roughly speaking: event = subset of sample space
Eg: E = {2,4,6} = outcome is even</p>
<p><strong>prob function</strong>: Notation $P$
prob function is a function of event &amp; it satisfies 3 conditions</p>
<ol class="list">
<li><p>$0\le P(E)\le 1$  for any event $E$</p>
</li>
<li><p>$P(sample ~space) = 1$</p>
</li>
<li><p>additivity
Suppose we have a sequence of disjoint events, $E_1, E_2, \ldots$, i.e. $E_i\cap E_j=\emptyset, i\neq j$, then $P(\bigcup_{i=1}^\infty E_i) = \sum_{i=1}^\infty P(E_i)$</p>
</li>
</ol>
<p>   Eg: toss a dice and define $P(E) = {\text{# of outcoms in E} \over 6} \implies P$ is prob function</p>
<p><strong>properties of prob function </strong></p>
<ol class="list">
<li>If $E_1 \subset E_2$, then $P(E_1) \leq P(E_2)$</li>
<li>If $E_1\cap E_2 = \emptyset$, then $P(E_1\cup E_2) = P(E_1) + P(E_2)$</li>
<li>$P(\emptyset) = 0$</li>
<li>$P(E)+P(E^C) = 1$ ($E^C$: complementary set of $E$)</li>
<li>$P(E_1\cup E_2) = P(E_1)+P(E_2)-P(E_1\cap E_2)$</li>
</ol>
<h4 id="**indepdence**"><a class="header-link" href="#**indepdence**"></a><strong>Indepdence</strong></h4>
<p>Suppose we have two events $E \&amp; F$. They are independent if $$\underbrace{P( E\cap F) }_{joint ~ prob} = P(E) * P(F)$$</p>
<p>The latter one is marginal probs or unconditional probs.</p>
<p>Independent $\implies$ joint = product of marginals.
More than two envents will not be covered.</p>
<p><strong>A simple and useful fact</strong></p>
<p>Suppose we have a sequence of independent trials and a sequence of events $E_1,E_2,\ldots$</p>
<p>Now: $E_i$ only depends on its trial</p>
<p>Then: $E_1, E_2,\ldots$ are independent
and $$\displaystyle P\left(\bigcap_{i=1}^\infty E_i\right) = \prod_{i=1}^\infty P(E_i) \qquad P\left(\bigcap_{i=1}^m E_i\right) = \prod_{i=1}^m P(E_i) $$</p>
<h1 id="lecture-3"><a class="header-link" href="#lecture-3"></a>Lecture 3</h1>
<h2 id="conditional-prob"><a class="header-link" href="#conditional-prob"></a>conditional prob</h2>
<p><strong>defn</strong>: Suppose we have two events: $E\&amp; F \&amp; P(F)&gt;0$, then $P(E|F) = \dfrac{P(E\cap F)}{P(F)}$</p>
<ul class="list">
<li>Result1: $P(E\cap F) = P(E|F) P(F)$ [multiplication rule]</li>
<li>Result2: If $E\&amp;F$ are independent, the $P(E|F)=P(E)$</li>
</ul>
<h2 id="bayesian-formula"><a class="header-link" href="#bayesian-formula"></a>Bayesian formula</h2>
<p>Suppose  we have a sequence of events $F_1,F_2,\ldots$, such that
$$
\begin{cases}
\bigcup_i F_i = \text{sample space} \
F_i\cap F_j = \emptyset &amp;i\neq j \
P(F_i)&gt;0
\end{cases}
$$
Then
$$
\begin{equation<em>}
\begin{split}
P(E) &amp;=\sum_iP(E\cap F_i) \
&amp;=\sum_i P(E|F_i)P(F_i) \
\end{split}
\implies
P(E)=\sum_iP(E|F_i) P(F_i)
\end{equation</em>}
$$
Law of total prob. [divide and conquer method]</p>
<p>Next:
$$
\begin{equation}
\begin{split}
P(F_k|E) &amp;= {P(E\cap F_k) \over P(E)} \
&amp;= {P(E|F_k) P(F_k) \over \sum_i P(E|F_i)P(F_i)}
\end{split}
\end{equation}
$$</p>
<h2 id="random-variables"><a class="header-link" href="#random-variables"></a>random variables</h2>
<p><strong>Def</strong>: A rv is a function defined on sample space to real time</p>
<p>X: sample space (domain) -&gt; real time (range)</p>
<h3 id="two-types-of-rvs-in-this-course"><a class="header-link" href="#two-types-of-rvs-in-this-course"></a>two types of rvs in this course</h3>
<ol class="list">
<li>Discrete: all possible values are at most countable (possion rv)</li>
<li>Conitinuous: all possible values contain an interval (exponential distribution)</li>
</ol>
<p>Next: important rvs</p>
<h1 id="lec-4"><a class="header-link" href="#lec-4"></a>Lec 4</h1>
<h2 id="bernulli-trails"><a class="header-link" href="#bernulli-trails"></a>Bernulli Trails</h2>
<p>(1) each trail has 2 outcomes: $S$ &amp; $F$.
(2) all trails are independent
(3) prob of $S$ on each trial are the same.</p>
<p><strong>Notation</strong> $p = Pr(<code>S&quot;) ~~ ~~q = 1-p=Pr(</code>F&quot;)$</p>
<p><strong>Bernoulli rvs</strong> Notation Bernoulli (p)</p>
<p>Let
$$
I_i=\begin{cases}
1 &amp; \text{if S appears on the ith trail} \
0 &amp; \text{otherwise}
\end{cases}
$$
Then $Pr(I_i=1)=p$ &amp; $P_r(I_i=0)=q$  &amp; $I_1,I_2,\ldots,I_n,\ldots$ are a sequence of iid (independent identically distributed) Bernoulli rvs.</p>
<h2 id="binomial-rv"><a class="header-link" href="#binomial-rv"></a>Binomial rv</h2>
<p>Notation $Bin(n,p)$</p>
<p>$X=$# of &quot;S&quot; in $n$ Bernoullli trials $\sim Bin(n,p)$
n = # of Bernoulli trials [fixed]          p is prob of &quot;S&quot;</p>
<p><strong>Range</strong> ${0,1,\ldots,n}$         $P(X=k)=\underbrace{\binom{n}{k}p^k(1-p)^{n-k}}_{\text{prob mass function}}$     $k=0,1,2,\ldots, n$</p>
<ul class="list">
<li>Result 1:$X=\sum_{i=1}^n I_i$ where $I_1,\ldots, I_n$ are iid Bernoulli rvs, $X$ is binomial</li>
<li>Result 2: If $X_1\sim Bin(n_1,p), X_2\sim Bin(n_2,p)$ and they are independent, then $X_1+X_2 \sim Bin(n_1+n+2,p)$</li>
</ul>
<p>Why?</p>
<p>$X_1=$ # of &quot;S&quot; in first $n_1$ trials (Bernoulli trials)
$X_2=$ # of &quot;S&quot; in next $n_2$ trials (Bernoulli trials)</p>
<p><strong>Indepdent</strong> no overlaps between two groups of trials</p>
<h2 id="geometric-rv"><a class="header-link" href="#geometric-rv"></a>Geometric rv</h2>
<p>Geo(p)</p>
<p>First waiting time rv</p>
<p>$X=$ # of <strong><em>trials</em></strong> to get first &quot;S&quot; in the sequence of Bernoulli trials</p>
<p>Range of $X={1,2,\ldots}$       $E[X]={1\over p}$</p>
<p>prob mas function $P(X=k)=(1-p)^{k-1} p$      $k=1,2,3,\ldots$</p>
<p><strong>property</strong> No-memory property</p>
<p>$P(X&gt;n+m|X&gt;m)=P(X&gt;n)=P(X-m&gt;n|X&gt;m)$</p>
<ul class="list">
<li>$X&gt;m$ : at time $m$ , we don&#39;t observe &quot;S&quot;</li>
<li>$X-m$ : remaining time</li>
</ul>
<p><strong>Formula tells us</strong>  given that we don&#39;t observe the event &quot;S&quot;, the remaining time $\sim Geo(p)$</p>
<p><strong>That is</strong> As long as we don&#39;t observe the event, remaining time and original time have same distribution $Geo(p)$ s</p>
<h1 id="lec-5"><a class="header-link" href="#lec-5"></a>Lec 5</h1>
<h2 id="negtive-binomial"><a class="header-link" href="#negtive-binomial"></a>Negtive Binomial</h2>
<p>$NegBin(r, p)$ &amp; $X=\sum_{i=1}^v X_i:$ $X_1, \ldots, X_v$ are iid Geo(p) rvs</p>
<h2 id="possion-rv:-pois-($\lambda$)"><a class="header-link" href="#possion-rv:-pois-($\lambda$)"></a>Possion rv: pois ($\lambda$)</h2>
<p>Range of X = {0, 1, 2, ...}</p>
<p>pmf: $P(X=k)= {\lambda^k e^{-\lambda} \over k!}$  $k=0,1,2,\ldots$</p>
<p><strong>Here</strong> $\lambda$ (1)rate parameter (2) $\lambda=E(X)$</p>
<p><strong>Property</strong> Suppose $X_1\sim pois(\lambda_1) \&amp; X_2\sim pois (\lambda_2)$ and they are independent, then $X_1+X_2 \sim pois(\lambda_1+\lambda_2)$</p>
<h2 id="exponential-rv:-$\exp(\lambda)$"><a class="header-link" href="#exponential-rv:-$\exp(\lambda)$"></a>Exponential rv: $\exp(\lambda)$</h2>
<p>The continuous waiting time rv.</p>
<p>probability density function (pdf):
$$
f(x)=\begin{cases}
\lambda e^{-\lambda x} &amp; x&gt;0 \
0 &amp; otherwise
\end{cases}
$$
<strong>properties </strong>: (a) $\lambda$: rate parameter = ${1\over expectation}$<br>               $E(x)={1\over \lambda}$ if $X\sim \exp(\lambda)$</p>
<p>​                 (b) tail prob: $P(X &gt;t) = e^{-\lambda t}\qquad t&gt;0$  (at time t, we don&#39;t observe the event)</p>
<p>​                    (c) No-memory property: missing pic</p>
<p><strong>No memory property meaning</strong> : As long as we don&#39;t observe the event, Remaining time $k$ original time both $\sim \exp(\lambda)$</p>
<h2 id="expectation-&-variance"><a class="header-link" href="#expectation-&-variance"></a>Expectation &amp; Variance</h2>
<p><strong>Discrete rv</strong>: $X$ with range ${x_0, x_1,\ldots, x_n, \ldots}$
Then $E(x)= \sum_{i=0}^\infty \underbrace{x_i}_{value} \times P(X=x_i)$</p>
<p><strong>Continous rv</strong>: $X$ with pdf f(x), then $\displaystyle E(x)= \int_{-\infty}^\infty x f(x)dx$</p>
<p><strong>General case</strong>: $E[g(x)]$       $g(x)$ is a real-function
$$
E[g(x)]=\begin{cases}
\sum_{i=0}^\infty g(x_i)P(X=x_i) &amp; \text{discrete case} \
\int_{-\infty}^\infty g(x) f(x)dx &amp; \text{continuous case}
\end{cases}
$$
<strong>variance</strong>: $var[X]=E[(X-E(X))^2] = E(X^2)-E^2(X)$</p>
<p><strong>covariance</strong> $cov(X,Y)=E[(X-E(X))(Y - E(Y))] = E(XY)-E(X)E(Y)$</p>
<p>If X and Y are independent, then $cov(X,Y)=0$</p>
<p><strong>Properties</strong>:</p>
<ol class="list">
<li><p>$E[\sum_{i=1}^n a_i X_i] = \sum_{i=1}^n a_i E(X_i)$   Linearity
$a_1, \ldots, a_n$ are constants and $X_1,\ldots, X_n$ are rvs</p>
</li>
<li><p>If $X_1, \ldots, X_n$ are independent, $var(\sum_{i=1}^n a_i X_i) = \sum_{i=1}^n a_i^2 ~~var(X_i)$</p>
</li>
<li><p>In general:
$$
var(\sum_{i=1}^n a_i X_i) = \sum{i=1}^n a_i^2 ~~var(X_i) + \sum_{i\neq j} a_i a_j ~cov (X_i, X_j) \
=\sum{i=1}^n a_i^2 ~~var(X_i) + 2\sum_{i&lt; j} a_i a_j ~cov (X_i, X_j)
$$</p>
</li>
</ol>
<h1 id="lec-6"><a class="header-link" href="#lec-6"></a>Lec 6</h1>
<h2 id="indicator-rvs"><a class="header-link" href="#indicator-rvs"></a>indicator rvs</h2>
<p><strong>Indication rv</strong> only two variables 0 &amp; 1</p>
<p>For a given event A, we define
$$
I_A =\begin{cases}
1 &amp; \text{if A occurs} \
0 &amp; otherwise
\end{cases}
$$
Let $P(A)=p \implies P(I_A=1)= p \qquad \&amp; \qquad P(I_A=0)=q=1-p$</p>
<h2 id="chapter-2-waiting-time-rcs"><a class="header-link" href="#chapter-2-waiting-time-rcs"></a>chapter 2 waiting time rcs</h2>
<p>Suppose we have a sequence of trials &amp; we are interested in observing event E based on trials.</p>
<p>$T_E$ = # of trials or waiting time to observe first E.</p>
<p>Range of $T_E=\underbrace{{1,2,\ldots}}<em>{we~can~observe~E}\cup \underbrace{{\infty}}</em>{we~can&#39;t}$</p>
<p>We are intersted in</p>
<ol class="list">
<li>If $P(T_E &lt; \infty) &lt; 1$ or $P(T_E=\infty)&gt;0$</li>
<li>If $P(T_E &lt; \infty) = 1$ , what is $E(T_E)$?</li>
</ol>
<p><strong>Classification of $T_E$</strong></p>
<ol class="list">
<li><p>If  $P(T_E &lt; \infty) &lt; 1$ , then $T_E$ is improper</p>
</li>
<li><p>If $P(T_E &lt; \infty) =1$, then $T_E$ is proper</p>
<p>2.1 If $E(T_E)=\infty$, $T_E$ is NULL proper</p>
<p>2.2 If $E(T_E)&lt;\infty$, $T_E$ is short proper</p>
</li>
</ol>
<h1 id="lec-7"><a class="header-link" href="#lec-7"></a>Lec 7</h1>
<ol class="list">
<li><p>If $T_E$ is improper, then $E(T_E)=\infty$  . since
$$
E(T_E) = \1<em>P(T_E=1)+2</em>P(T_E=2)+\ldots+\underbrace{\infty*P(T_E=\infty)}_{=\infty} = \infty
$$</p>
</li>
<li><p>If $E(T_E)&lt;\infty$, then $T_E$ is short proper, we do not need to verify $P(T_E&lt;\infty)=1$</p>
</li>
</ol>
<p><strong>Aside</strong>
$$
\sum_{n=1}^\infty a_n = \lim_{m\to\infty} \sum_{n=1}^m a_n \
\prod_{n=1}^\infty a_n = \lim_{m\to\infty}\prod_{n=1}^ma_n
$$
Note, in $\sum_{n=1}^\infty a_n \&amp; \prod_{n=1}^\infty a_n $, we do not include $\infty$ term. Hence, $P(T_E&lt;\infty)= \sum_{n=1}^\infty P(T_E=n)$</p>
<h2 id="chapter-3-conditional-expectation"><a class="header-link" href="#chapter-3-conditional-expectation"></a>Chapter 3 Conditional expectation</h2>
<h3 id="3.1-joint-rvs"><a class="header-link" href="#3.1-joint-rvs"></a>3.1 joint rvs</h3>
<p>We consider 2rvs</p>
<h4 id="joint-discrete:-x-&-y"><a class="header-link" href="#joint-discrete:-x-&-y"></a>Joint discrete: X &amp; Y</h4>
<p>If X &amp; Y are two discrete rvs, then X &amp; Y together is called joint discrete.</p>
<ul class="list">
<li><p>Joint pmf: $f_{X,Y}(x,y)=P(X=x, Y=y)$</p>
</li>
<li><p>Properties</p>
<ul class="list">
<li>Joint pmf is pmf $\begin{cases} f_{X,Y}(x,y)\ge 0 \ \sum_x \sum_y  f_{X,Y}(x,y) = 1\end{cases}$</li>
</ul>
<p>*</p>
<ul class="list">
<li>$$
f_X(x)=P(X=x)=\sum_y f_{X,Y}(x,y) \qquad \text{marginal pmf of X}
\
f_Y(y)=P(Y=y)=\sum_x f_{X,Y}(x,y) \qquad \text{marginal pmf of Y}
$$</li>
</ul>
</li>
<li><p>Joint expectation: $E[h(X,Y)]$    $h(x,y)$ is a real function
$E[h(X,Y)]=\sum_x\sum_y h(x,y)*f_{X,Y}(x,y)$</p>
</li>
</ul>
<p><strong>EG</strong>  $E(XY)=\sum_x\sum_y xy f_{X,Y}(x,y)$
       $E(X)= \sum_x\sum_y x f_{X,Y}(x,y) = \sum_x x f_X(x)$</p>
<h4 id="joint-continuous"><a class="header-link" href="#joint-continuous"></a>joint continuous</h4>
<p>If X &amp; Y are two continuous rvs and
$$
P(X\le x, Y\le y)=\int_{-\infty}^x\left[\int_{-\infty}^y f_{X,Y}(s,t) dt\right] ds
$$
<strong>properties</strong></p>
<ol class="list">
<li><p>$f_{X,Y}(x,y)$ is a pdf</p>
<ul class="list">
<li>$f_{X,Y}(x,y) \ge 0$</li>
<li>$\int_{-\infty}^\infty\int_{-\infty}^\infty f_{X,Y}(x,y) dxdy = 1$</li>
</ul>
</li>
<li><p>$\text{}$
$$
f_X(x)=\int_{-\infty}^\infty f_{X,Y}(x,y) dy \qquad marginal~pdf~of~X \
f_Y(y)=\int_{-\infty}^\infty f_{X,Y}(x,y) dx \qquad marginal~pdf~of~Y
$$</p>
</li>
</ol>
<p><strong>expectation</strong>
$$
E[h(X,Y)]=f_X(x)=\int_{-\infty}^\infty \int_{-\infty}^\infty h(x,y) f_{X,Y}(x,y) dx dy \
E[X]=\int_{-\infty}^\infty \int_{-\infty}^\infty x f_{X,Y}(x,y) dx dy= \int_{-\infty}^\infty x f_X(x)dx
$$
<strong>independence</strong> both discrete and continuous</p>
<p>If $f_{X,Y}(x,y)=f_X(x)f_Y(y)$, then X and Y are independent. i.e. joint = product of marginal</p>
<p><strong>property</strong> If X and Y are indepdent, $g(X)\&amp; h(Y)$ are independent</p>
<h1 id="lec-8"><a class="header-link" href="#lec-8"></a>Lec 8</h1>
<p><strong>property</strong></p>
<ol class="list">
<li>If X &amp; Y are independent, then $g(X)\&amp;h(Y)$ are independent.</li>
<li>If X &amp; Y are independent, then $E[g(X)h(Y)]=E[g(X)]E[h(Y)]$</li>
</ol>
<p><strong>Note</strong> $Cov(X,Y)=0$ does not imply X &amp; Y are independent.</p>
<h2 id="3.2-conditional-distribution-&-conditional-expectation"><a class="header-link" href="#3.2-conditional-distribution-&-conditional-expectation"></a>3.2 Conditional distribution &amp; Conditional expectation</h2>
<ul class="list">
<li><p>discrete case: notation: $f_{X,Y}(x,y)\to$ joint pmfs</p>
</li>
<li><p><strong>Def</strong> For a given $y$, the conditional pmf of $x$ given $Y=y$ is
$$
f_{X|Y}(x|y)={f_{X,Y}(x,y)\over f_Y(y)}={Joint\over marginal} \quad f_Y(y)&gt;0
$$</p>
</li>
<li><p>Property: $f_{X|Y}(x|y)$ is a pmf
<em>That is</em> (1) $f_{X|Y}(x|y)\ge 0$,  (2) $\sum_x f_{X|Y}(x|y) = 1$</p>
</li>
</ul>
<p>Proof is trivial</p>
<p><strong>Conditional Expectation</strong></p>
<p>The conditional expectation of $x$ given $Y=y$ is
$$
E(X|Y=y)=\sum_x<em>f_{X|Y}(x|y)
$$
The conditional expectation of $g(x)$ given $Y=y$ is
$$
E[g(x)|Y=y]=\sum_x g(x)</em>f_{X|Y}(x|y)
$$</p>
<h1 id="lec-9"><a class="header-link" href="#lec-9"></a>Lec 9</h1>
<h3 id="properties-of-conditional-expectation"><a class="header-link" href="#properties-of-conditional-expectation"></a>properties of conditional expectation</h3>
<ol class="list">
<li><p>Conditional expectation has all properties of expectation. Eg
$$
E(\sum_{i=1}^na_iX_i|Y)=\sum_{i=1}^na_iE(X_i|Y)
$$</p>
</li>
<li><p>Substitution rule
$$
E[X<em>g(Y)|Y=y]=E[X</em>g(y)|Y=y]\=g(y)<em>E(X|Y=y)
$$
Eg: $E(X</em>Y|Y=y)=y*E(X|Y=y)$</p>
<p>In general: $E(h(X,Y)|Y=y)=E(h(X,y)|Y=y)$</p>
</li>
<li><p>Independence property. If X &amp; Y are independent, then
$$
f_{X|Y}(x|y)={f_{X,Y}(x,y)\over f_Y(y)}={f_X(x)f_Y(y)\over f_Y(y)}=f_X(x)
$$</p>
<p>$$
\implies E(X|Y=y)=E(x)~~\&amp;\ E[g(X)|Y=y]=E[g(X)]
$$</p>
</li>
</ol>
<h2 id="3.3-calculating-expectation-by-conditioning"><a class="header-link" href="#3.3-calculating-expectation-by-conditioning"></a>3.3 Calculating expectation by conditioning</h2>
<p>This section we cover: $E(X)=E[E(X|Y)]$  </p>
<p>​                Law of total expectation/double expectation</p>
<p><strong>step 1</strong> what is $E(X|Y)?$</p>
<p>(a) $E(X|Y)$ is a rv &amp; dependents on $Y$. say $E(X|Y)=g(Y)\to$ $E(X|Y)$ depends on $Y$.</p>
<p>(b) given $Y=y$, the function of $g(Y)$, $\underbrace{g(y)=E(X|Y=y)}_{\text{in section 3.2}}$</p>
<p>​    <strong>eg</strong> eg3.1: $E(X|Y=y)=y{\lambda_1\over \lambda_1+\lambda_2}\implies g(y)=y{\lambda_1\over \lambda_1+\lambda_2}$</p>
<p>​          eg3.2: $E(X|Y=y)={2\over y}\implies g(y)={2\over y}$</p>
<p><strong>step2</strong> How to get $E(X|Y)$?</p>
<p>(a) figure out $g(y)=E(X|Y=y)$ first by definitions/by properties</p>
<p>(b) $E(X|Y)$ is just $g(Y)$.</p>
<p><strong>step 3</strong> how to apply $E(X)=E(E(X|Y))$
$$
E(X)=E(E(X|Y))=E[g(Y)]\
=\begin{cases}
\sum_y g(y)<em>f_Y(y) &amp; discrete~case\
\int_{-\infty}^\infty g(y)</em>f_Y(y)dy &amp; continuous~case
\end{cases}
\
=\begin{cases}
\sum_y E(X|Y=y)f_Y(y) &amp; discrete~case\
\int_{-\infty}^\infty E(X|Y=y)*f_Y(y)dy &amp; continuous~case
\end{cases}
$$
<strong>comments:</strong> $E(X)=E[E(X|Y)]$ is applicable to all expectations</p>
<h1 id="lec-10"><a class="header-link" href="#lec-10"></a>Lec 10</h1>
<h2 id="3.4-computing-prob-by-conditioning"><a class="header-link" href="#3.4-computing-prob-by-conditioning"></a>3.4 computing prob by conditioning</h2>
<p>Suppose A is an event and we are interested in $P(A)$</p>
<p>Let $I_A= \begin{cases} 1 &amp; \text{if A occurs } \ 0 &amp;\text{otherwise}\end{cases}$
$$
P(A)=E(I_A)=E(E(I_A|Y))\= \begin{cases}
\sum_y E(I_A|Y=y)f_Y(y)&amp;discrete~Y \
\int_{-\infty}^\infty E(I_A|Y=y)f_Y(y)dy &amp; continuous~Y
\end{cases} \
=\begin{cases}
\sum_y E(A|Y=y)f_Y(y)&amp;discrete~Y \
\int_{-\infty}^\infty E(A|Y=y)f_Y(y)dy &amp; continuous~Y
\end{cases}
$$</p>
<h1 id="lec-12"><a class="header-link" href="#lec-12"></a>Lec 12</h1>
<h2 id="3.5-calculating-variance-by-conditioning"><a class="header-link" href="#3.5-calculating-variance-by-conditioning"></a>3.5 Calculating variance by conditioning</h2>
<p><strong>Method1</strong> By defn</p>
<p>Recall
$$
Var(X)=E(X^2)-[E(X)]^2\
E(X^2)=E[E(X^2|Y)]\
E(X)=E[E(X|Y)]
$$
This method has been covered before.</p>
<p><strong>Method2</strong> Conditional variance.</p>
<p>Given $Y=y$, the coditional variance of $X$ is given as
$$
Var(X|Y=y)=E(X^2|Y=y)-[E(X|Y=y)]^2
$$
<strong>Note</strong> $Var(X|Y=y)$ depends on $y$ and is a function $y$, say
$$
Var(X|Y=y)=h(y)
$$
<strong>EG</strong> $X|Y=y \sim pois(y) \implies Var(X|Y=y)=y \implies h(y)=y$
Then, apply $h(y)$ to $Y$, we get a r.v. This rv is denoted as $Var(X|Y)=h(Y)$</p>
<p>Basically: $Var(X|Y)=h(Y)=E(X^2|Y)-[E(X|Y)]^2$ conditional variance variance of $X$ given $Y$.</p>
<p>Two steps to find $Var(X|Y)$</p>
<ul class="list">
<li>Step1: find $h(y)=Var(X|Y=y)$</li>
<li>Step 2: apply $h(y)$ to $Y$ to get $var(X|Y)=h(Y)$</li>
</ul>
<p>Comments:</p>
<ol class="list">
<li>Substitution rule is still applcable</li>
<li>If X &amp; Y are independent, then $var(X|Y=y)=var(X)$</li>
</ol>
<p><strong>THEOREM</strong>
$$
Var(X)=E[\underbrace{var(X|Y)}_{\ge 0}]+var[E(X|Y)]
$$
From $X$ to $E(X|Y)$</p>
<p>1st $E(X)=E(E(X|Y))$</p>
<p>2nd $Var(X)\ge Var(E(X|Y))$</p>
<p><strong>PROOF</strong></p>
<p><em>LHS</em> $Var(X)=E(X^2)-[E(X)]^2$
$$
\begin{split}
RHS =&amp; E(var(X|Y)) + var[E(X|Y)]\
=&amp; E[E(X^2|Y) - \cancel{ {E(X|Y}^2}] \&amp;+ E[\cancel{{E(X|Y}^2}}]-{E[E(X|Y)]}^2 \
=&amp; E(X^2)-[E(X)]^2=LHS
\end{split}
$$</p>
<p><strong>Method3</strong> Compound rv formula: [random sum of iid rvs]</p>
<p>Suppose $X_1, X_2,\ldots, X_n, \ldots$ are a sequence of iid res.</p>
<p>N: a rv only takes non-negative integers.</p>
<p><strong>Further</strong> N &amp; $X_1, \ldots$ are independent.</p>
<p>Then $W=\sum_{i=1}^N X_i$: compound rv.  [If $N=0$, then $W=0$]</p>
<p><strong>Thm</strong>: $E(W)=E(N)<em> E(X_1) \quad \&amp; \quad var(W)=E(N)</em> var(X_1)+var(N)*[E(X_1)]^2$</p>
<p><strong>PROOF</strong>
$$
E(W)=E(E(W|N))\
\begin{split}
E(W|N=n)&amp;= E\left(\sum_{i=1}^NX_i|N=n\right)\
&amp;=E\left(\sum_{i=1}^nX_i|N=n\right)\
&amp;=E\left(\sum_{i=1}^nX_i\right)\
&amp;= n*E(X_1)
\end{split}
$$</p>
<p>$$
\implies E(W|N)=N<em>E(X_1)\
\implies E(W)=E(E(W|N))=E(N</em>E(X_1))=E(N)<em>E(X_1)\
Var(W)=E[Var(W|N)]+Var[\underbrace{E(W|N)}_{N</em>E(X_1)}]
$$</p>
<p>$Var(W|N)?$
$$
\begin{aligned}
Var(W|N=n)&amp;= var\left(\sum_{i=1}^N X_i |N=n\right) \
&amp;= var\left(\sum_{i=1}^n X_i |N=n\right) \
&amp;= var\left( \sum_{i=1}^n X_i\right) \quad X_1,\ldots, X_n\&amp;N~indepdent \
&amp;= \sum_{i=1}^n var(X_i) \qquad X_1,\ldots, X_n~are~i.i.d\
&amp;=n*var(X_1)
\end{aligned}
$$</p>
<p>$$
\implies Var(W|N)=N*var(X_1)
$$</p>
<p><strong>Next</strong>
$$
\begin{split}
Var(W)&amp;=E[var(W|N)]+var[E(W|N)]\
&amp;=E[N<em>var(X_1)]+var(N</em>E(X_1))\
&amp;=E(N)<em>var(X_1)+var(N)</em>[E(X_1)]^2
\end{split}
$$</p>
<h1 id="lec-13"><a class="header-link" href="#lec-13"></a>Lec 13</h1>
<p>examples &amp; advice on midterm1</p>
<h2 id="ch-4-probability-generating-function-(pgf)"><a class="header-link" href="#ch-4-probability-generating-function-(pgf)"></a>ch 4 probability generating function (pgf)</h2>
<h3 id="4.1-generating-function-(gf)"><a class="header-link" href="#4.1-generating-function-(gf)"></a>4.1 generating function (gf)</h3>
<p><strong>defn</strong> given a sequence of real # $s$ ${a_n}<em>{n=0}^\infty$, define
$$
A(s)=\sum</em>{n=0}^\infty a_n s^n
$$</p>
<h3 id="power-series"><a class="header-link" href="#power-series"></a>power series</h3>
<p>According to values of ${a_n}_{n=0}^\infty$, we have 3 situations:</p>
<ol class="list">
<li>$A(s)$ converges only at $s=0$</li>
<li>$A(s)$ converges when $|s|&lt;R$ for some $R&gt;0$ &amp; diverges when $|s|&gt;R$.</li>
<li>$A(s)$ converges when $|s|&lt;\infty$, here $R=\infty$</li>
</ol>
<p>When we have cases 2 &amp; 3, $A(s)$ is called generating function of ${a_n}_{n=0}^\infty$ and $R$ is called convergence radius</p>
<p><strong>eg</strong> (1) $a_n=1$ for $n\ge 0$, $A(s)=\sum_{n=0}^\infty s^n = \begin{cases}{1\over 1-s} &amp; |s|<1 \\ diverges & |s|>1 \end{cases} \implies R=1$</p>
<p>(2) $a_n={1\over n!}$ for $n\ge 0$, $A(s)=\sum_{n=0}^\infty {1\over n!}s^n=e^s$ for $|s|&lt;\infty \implies R=\infty$</p>
<p><strong>THeorem</strong> There is a one-to-one correspondence between ${a_n}_{n=0}^\infty$ and $A(s)$</p>
<p>(1) Given ${a_n}_{n=0}^\infty$, $A(s)$ is uniquely defined</p>
<p>(2) Given $A(s)$, then ${a_n}_{n=0}^\infty$ is uniquely determined.</p>
<p><strong>uniquely determined</strong> given $A(s)$ $\begin{cases}a_0=A(0) \ a_n={A^{(n)}(0) \over n!} &amp; for ~n\ge 1 \end{cases}$</p>
<h4 id="commonly-used-power-series"><a class="header-link" href="#commonly-used-power-series"></a>commonly used power series</h4>
<ol class="list">
<li>Geometric: $A(s)=\sum_{n=0}^\infty s^n = {1\over 1-s}, \quad R=1$. $a_n=1$ for $n\ge 0$</li>
<li>Altenative Geometric $A(s)=\sum_{n=0}^\infty (-1)^ns^n = {1\over 1+s}$   $R=1$.  $a_n=(-1)^n$  for $n\ge 0$.</li>
<li>Exponential $A(s)=\sum_{n=0}^\infty {1\over n!}s^n$  $R=\infty$.  $a_n={1\over n!}, n\ge 0$</li>
<li>Binomial $A(s)=(1+s)^n = \sum_{R=0}^n\binom{n}{R}s^R$
$\begin{cases}a_R=\binom{n}{R} &amp; R=0,1,2\ldots, n\a_k=0 &amp; R\ge n+1\end{cases}$
$n$ is postive integer, $R=\infty$</li>
<li>general binomial
$(1+s)^\alpha = \sum_{n=0}^\infty\binom{\alpha}{n}s^n$,  $\alpha$ is a real # (not postive integer)
$\binom{\alpha}{n}={\alpha(\alpha-1)\ldots(\alpha-n+1)\over n!}$  &amp;  $R=1$</li>
</ol>
<h1 id="lec-14"><a class="header-link" href="#lec-14"></a>Lec 14</h1>
<p>$$
\binom{-1\over 2}{n}=\left(-{1\over 4}\right)^n\binom{2n}{n}
$$</p>
<p>Properties of gf
$$
A(s)=\sum_{n=0}^\infty a_n s^n \qquad B(s)=\sum_{n=0}b_ns^n
$$
Let $R_A, R_B$ be convergence radius</p>
<ol class="list">
<li><p>Summation
$$
C(s)=A(s)+B(s)=\sum_{n=0}^\infty a_n s^n + \sum_{n=0}^\infty s^n =\sum_{n=0}^\infty(a_n+b_n)s^n
$$
$c_n = a_n+b_n$     $R_C = \min(R_A,R_B)$
$$
C(s)=A(s)-B(s)=\sum_{n=0}^\infty(a_n-b_n)s^n
$$
$c_n=a_n-b_n$     $R(c)=\min(R_A,R_B)$</p>
</li>
<li><p>Product
$$
C(s)=A(s)*B(s)=\sum_{n=0}^\infty c_ns^n
$$
$c_n=\underbrace{\sum_{k=0}^n a_kb_{n-k}}_{n+1~terms} \ne a_nb_n$</p>
<p>$R_C=\min (R_A,R_B)$</p>
<p>That is
$$
\left(\sum_{n=0}^\infty a_n s^n\right)\left(\sum_{n=0}^\infty b_n s^n\right)
=\sum_{n=0}^\infty \left(\sum_{k=0}^n a_k b_{n-k}s^n\right)
$$
[convolution of $A(s)$ &amp; $B(s)$]</p>
</li>
</ol>
<p><strong>e.g.</strong> find ${c_n}_{n=0}^\infty$ &amp; $R_C$</p>
<ol class="list">
<li>$C(s)={1\over 1-s}*{1\over 1+s}$     2. $C(s)={1\over (1-s)^2}$</li>
</ol>
<p><strong>soln</strong></p>
<p>(1) $C(s)={1\over 2}\left[{1\over 1-s}+{1\over 1+s}\right]={1\over 2}[a_n+b_n]$
then $a_n=1, R_A=1$, $b_n=(-1)^n, R_B=1$  $n\ge 0$
$C(s)={1\over 2}\sum_{n=0}^\infty (1+(-1)^n)s^n$
Hence
$$
\begin{cases}
c_n={1\over 2}(1+(-1)^n)&amp; n\ge 0\
R_C=1 = \min(R_A,R_B)
\end{cases}
$$
(2)</p>
<ul class="list">
<li><p>Method1: $C(s)={1\over 1-s}{1\over 1-s}$
then $B(s)=A(s)=\sum_{n=0}^\infty s^n, \quad a_n=b_n=1, R_A=R_B=1, n\ge 0$</p>
<p>$c_n=\sum_{k=0}^n a_k b_{n-k}=n+1$  $R_C=\min(R_A,R_B)=1$</p>
</li>
<li><p>Method2: $C(s)=(1+(-s))^{-2} = \sum_{n=0}^\infty (-1)^n\binom{-2}{n}s^n$
$$
\begin{split}
c_n &amp;= (-1)^n\binom{-2}{n}, \quad R_C=1\
&amp;= (-1)^n {(-2)(-2-1)\ldots(-2-n+1)\over n!}\
&amp;=(-1)^n(-1)^n {2<em>3</em>\ldots*(n+1)\over n!}=n+1
\end{split}
$$</p>
</li>
</ul>
<h2 id="4.2-probablity-generating-function-(pgf)"><a class="header-link" href="#4.2-probablity-generating-function-(pgf)"></a>4.2 Probablity generating function (pgf)</h2>
<p><strong>Defn</strong> Suppose $x$ is non-negative integer rv with range =${0,1,2,\ldots}\cup{\infty}$. Let $P_n=P(x=n)$ for $n=0,1,2,\ldots$</p>
<p>Then $G_X(s)=\sum_{n=0}^\infty P_ns^n = \sum_{n=0}^\infty P(X=n)s^n$ is called pgf of $X$</p>
<p>If $X$ is a proper rv,  then $\begin{cases}P(x=\infty)=0\P(x&lt;\infty)=1\end{cases}$  , then $G_X(s)=\sum_{n=0}^\infty P(X=n)s^n=E[s^X]$</p>
<p><strong>Comments</strong></p>
<ol class="list">
<li>$G_X(1)=\sum_{n=0}^\infty P(X=n)=P(X&lt;\infty) \le 1$
$$
|G_X(s)|=\left|\sum_{n=0}^\infty p_n s^n \right|\le \sum_{n=0}^\infty p_n |s|^n \le \sum_{n=0}^\infty p_n &lt;\infty
$$
if $|s|\le 1$</li>
</ol>
<p><strong>Property of pgf</strong></p>
<ol class="list">
<li><p>why pgf?
If we have $G_X(s)$, we can recover ${P_n=P(X=n)}_{n=0}^\infty$
Method 1:
$\begin{cases}P_0=G_X(0) \ P_n={G_X^{(n)}(0)\over n! }&amp;n\ge 1\end{cases}$
Method 2: use properties of gfs &amp; commonly used gfs to recover ${P_n}_{n=0}^\infty$</p>
<p>Reason for pgf:</p>
<ol class="list">
<li>pgf helps to find $P_n = P(X=n)$ for rv $x$ [discrete rv]</li>
<li>mgf(moment generating function) helps to find $E(X^k)$ for $k\ge 1$. [for both discrete &amp; discrete continuous rvs]</li>
</ol>
</li>
<li><p>Property 2
we can check if  $X$ is proper or not.
Note: $P(x&lt;\infty)=\sum_{n=0}^\infty P(X=n)=\sum_{n=0}^\infty P_n=G_X(1)$</p>
<pre class="hljs"><code>        <span class="hljs-built_in">and</span> Recall $G_X(s)=\sum_{<span class="hljs-built_in">n</span>=<span class="hljs-number">0</span>}^\infty P_ns^<span class="hljs-built_in">n</span>$</code></pre><p>​            If $G_X(1)=1\implies $proper.</p>
<p>​            If $G_X(1)&lt;1 \implies$ improper</p>
<p>​            If $G_X(1)&gt;1\implies$ you did sth wrong</p>
</li>
</ol>
<h1 id="lec-15"><a class="header-link" href="#lec-15"></a>Lec 15</h1>
<ol start="3">
<li><p>Property #3
If $x$ is proper, then
$$
E(x)= G_x&#39;(1)\
var(x)=\underbrace{G_x&#39;&#39;(1)+G_x&#39;(1)}_{E(x^2)}-[\underbrace{G_x&#39;(1)}_{E(x)}]^2
$$</p>
</li>
<li><p>Property #4: Uniqness Theorem
Two random variables X and Y have the same distribution iff $G_X(s)=G_Y(s)$  $\implies$ use pgf to determine distribution type</p>
</li>
<li><p>Property #5 Indepedence Property
If X and Y are independent with ranges = {0, 1, 2, …} $\cup {\infty}$</p>
<p>Argument:
$$
\begin{aligned}
G_{X+Y}(s)&amp;\stackrel{proper}{=}E[s^{X+Y}]\
&amp;=E[s^X<em>s^Y]\
&amp;\stackrel{indepdent}{=}E[s^X]E[s^Y] \
&amp;= G_X(s)</em>G_Y(s)
\end{aligned}
$$</p>
</li>
</ol>
<h1 id="lec-16"><a class="header-link" href="#lec-16"></a>Lec 16</h1>
<p>useless examples</p>
<h2 id="4.3-simple-random-walk"><a class="header-link" href="#4.3-simple-random-walk"></a>4.3 Simple random walk</h2>
<p>Background:</p>
<ul class="list">
<li>suppose we have a particle staring from $x_0$ (eg, $x_0=0$)</li>
<li>At each step, the particle $\begin{cases}\text{can move to right by 1 unit with prob $= p$}\
\text{can move to right by 1 unit with prob $q=1- p$} \end{cases}$</li>
</ul>
<p><strong>eg</strong> toss a coin</p>
<ul class="list">
<li>H: move to right by 1 unit</li>
<li>T: move to left by 1 unit</li>
</ul>
<p>Suppose we toss a coin 5 times and get HHTTH</p>
<p><strong>Def</strong> Simple random walk: Let $x_0$ be the starting point of the process (eg: $x_0=0$) &amp; $x_n$ be the position of the process after $n$ steps. Then ${x_n}_{n=0}^\infty$ is called a simple random walk or ordinary random walk.</p>
<p><strong>Our interest</strong></p>
<p>$\lambda_{0,0}=$ Returning to 0, given the process stars with 0</p>
<p>$\lambda_{0,k}=$ visiting $k$, given the process starts with 0</p>
<p><strong>LET</strong> $T_{0,0}=$ waiting time for observing 1st $\lambda_{0,0}$  $=\min{n\ge 1, x_n=0| x_0=0 }$</p>
<p>For example, $k=1$, in the example above, $T_{0,1}=1$.</p>
<p>We would like to find</p>
<ol class="list">
<li>$P(T_{0,0}&lt;\infty) \&amp; E(T_{0,0})$</li>
<li>$P(T_{0,k}&lt;\infty) \&amp; E(T_{0,k})$</li>
</ol>
<h1 id="lec-17"><a class="header-link" href="#lec-17"></a>Lec 17</h1>
<ul class="list">
<li>$T_{0,0}=$ waiting time for observing 1st $\lambda_{0,0}$  $=\min{n\ge 1, x_n=0| x_0=0 }$</li>
<li>$T_{0,k}=$ waiting time for observing 1st $\lambda_{0,k}$  $=\min{n\ge 1, x_n=0| x_0=0 }$</li>
</ul>
<p><strong>Notation</strong>
$$
G_{0,0}(s)=\sum_{n=0}^\infty P(T_{0,0}=n) s^n ~~ pgf~of ~T_{0,0} \
G_{0,k}(s)=\sum_{n=0}^\infty P(T_{0,k}=n) s^n ~~ pgf~of ~T_{0,k}
$$
Preparation: 1st for $k&gt;0$, positive integer $k$</p>
<p>$ T_{0,k}=T_{0,1}+T_{1,2}+\ldots+T_{k-1,k}$</p>
<p>$T_{i,j}$ = waiting time for visiting $j$, starting from $i$ = $\min{n\ge 1, x_n=j | x_0 = i}$</p>
<p><strong>Claim</strong> $T_{0,1}, T_{1,2},\ldots, T_{k-1,k}$ are $k$ iid rvs since all mean moving to right by 1 unit.</p>
<p><strong>That is</strong> $T_{0,k}=\sum_{i=1}^k T_{i-1,i}$             all $T_{i-1,i}$ are iid &amp; have same distribution as $T_{0,1}$.
$$
\implies G_{0,k}(s)=pgf~of~T_{0,k} = \prod_{i=1}^k \underbrace{G_{T_{i-1,i}}(s)}<em>{pgf~of~T</em>{i-1,i}} \
\implies G_{0,k}(s)=[G_{0,1}(s)]^k \qquad \text{for $k&gt;0$}
$$
2nd: In general:</p>
<ul class="list">
<li>$T_{0,1}\&amp; T_{-1,0}$ have same distribution [move to right by 1 unit]</li>
<li>$T_{1,0}\&amp;T_{0,-1}$  … [left by 1]</li>
</ul>
<p>they are all in simple random walk.</p>
<p><strong>Next</strong> move to pgf of $T_{0,1}$ then $T_{0,k}$ for $k&gt;0$.</p>
<p><strong>by def</strong> $G_{0,1}(s)=\sum_{n=0}^\infty P(T_{0,1}=n)s^n = pgf~of~ T_{0,1}$</p>
<p>$n=0, P(T_{0,1}=0)=0;~~ n=1, P(T_{0,1}=1)=P(\text{move to right by 1})=p$</p>
<p>For $n\ge 2$
$$
\begin{aligned}
P(T_{0,1}=n) &amp;\stackrel{\substack{\text{condition on}\\text{1st argument}}}{=}\
&amp;\underbrace{P(T_{0,1}=n|\text{1st stop = right})}_0 <em>
\underbrace{P(\text{1st stop = right})}<em>p \
&amp;+\underbrace{P(T</em>{0,1}=n|\text{1st stop = left})}_0 </em>
\underbrace{P(\text{1st stop = left})}_1
\end{aligned}
$$</p>
<p>1st stop = right $\implies T_{0,1}=1\ne n$  for $n\ge 2$
1st stop = left $\implies$ we are in position &quot;-1&quot; &amp; $T_{0,1}=n$ means $T_{-1,1}=n-1$</p>
<p>Hence $P(T_{0,1}=n)=P(T_{-1,1}=n-1)<em>q = P(T_{0,2}=n-1)</em>q$</p>
</div>
